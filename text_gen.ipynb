{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_gen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXBpB6-xehlF",
        "colab_type": "text"
      },
      "source": [
        "Let us import all the required libraries\n",
        "* We'll be using tensorflow and keras for our Deep Learning Networks.\n",
        "* We're using numpy to work with arrays.\n",
        "* Pandas to work with our data.\n",
        "* The re library to work with text using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h0HOmrcClME",
        "colab_type": "code",
        "outputId": "5a06c8c5-cb7e-4310-b3f6-62c92767c29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow import set_random_seed\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, GRU, Dense, Dropout, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-it7Wd1dCp_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "import pandas as pd\n",
        "import re\n",
        "from random import shuffle\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x87y5Hl3f2to",
        "colab_type": "text"
      },
      "source": [
        "We'll be setting up random seeds to reproduce our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUaHwO3d5j_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed(1)\n",
        "set_random_seed(2)\n",
        "random.seed(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z86LWc7hf9se",
        "colab_type": "text"
      },
      "source": [
        "Since we were using colabs, we need to import our data from our drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hij1TJAEC-NW",
        "colab_type": "code",
        "outputId": "71e433ac-5324-4454-ec86-7d89814094c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5TPUEGygKAJ",
        "colab_type": "text"
      },
      "source": [
        "Let's begin reading the lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0468Sr26FX7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines_df = pd.read_csv('bible.txt', sep='\\n', header=None, engine='python')\n",
        "lines_df = lines_df.iloc[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U4Wl0pQiJ3p",
        "colab_type": "text"
      },
      "source": [
        "Let us take a look at what we've got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt2K7nMWiNuG",
        "colab_type": "code",
        "outputId": "03eb9dda-9f43-4510-87d3-9666a2100485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(lines_df.iloc[3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1:2 And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfDbaf6kSRD",
        "colab_type": "text"
      },
      "source": [
        "Now let us begin by cleaning the texts.\n",
        "We'll be removing all punctuation, digits and special characters, keeping only the text and whitespace.\n",
        "\n",
        "\n",
        "We'll also shuffle our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcaSUmeqFc49",
        "colab_type": "code",
        "outputId": "5a79daef-e676-4b54-feb5-edbd5c2b41f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "clean_lines = lines_df.apply(lambda x: clean_text(x))\n",
        "print(clean_lines[0])\n",
        "shuffle(clean_lines)\n",
        "print(clean_lines[0])\n",
        "print(f\"Number of lines are {len(clean_lines)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the old testament of the king james version of the bible\n",
            " and their father said unto them what way went he for his sons had seen what way the man of god went which came from judah\n",
            "Number of lines are 24608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiUmj4s8liSa",
        "colab_type": "text"
      },
      "source": [
        "Now we'll convert our text in sequences and then using the N-gram model convert it into a sequence of N-words.\n",
        "\n",
        "An N-gram is a sequence of N words: a 2-gram (or bigram) is a two-word sequence of words like “look over”, “over there” and a 3-gram (or trigram) is a three-word sequence of words like “look over there”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRGGXtMhFeO-",
        "colab_type": "code",
        "outputId": "03c15f7c-a32f-474e-d8cf-5bd2831b3fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_tokens(text):\n",
        "    \n",
        "    tokenizer.fit_on_texts(text)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    input_sequences = []\n",
        "    for line in text:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "text_sequences, total_words = get_sequence_tokens(clean_lines[:1000])\n",
        "print(text_sequences[:5])\n",
        "print(tokenizer.sequences_to_texts(text_sequences[:5]))\n",
        "print(f\"Length of sequences array {len(text_sequences)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 37], [2, 37, 131], [2, 37, 131, 38], [2, 37, 131, 38, 11], [2, 37, 131, 38, 11, 21]]\n",
            "['and their', 'and their father', 'and their father said', 'and their father said unto', 'and their father said unto them']\n",
            "Length of sequences array 30440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MMloJ7XJm9y",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the example above, the sentence is converted into N number of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJSyRXHQJvxZ",
        "colab_type": "text"
      },
      "source": [
        "Now while feeding data, we require the number of inputs to be constant.\n",
        "\n",
        "However every sentence will not have the same number of words in it.\n",
        "\n",
        "Hence, inorder to adjust for this varying length we pad the sentences (pre or post).\n",
        "\n",
        "The maximum sequence size is defined by the maximum length in the input_sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsCvCzapFmdR",
        "colab_type": "code",
        "outputId": "7b3b60d5-5fb9-4b9f-ebed-b3972d4ce3c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_length = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences,\n",
        "                                             maxlen=max_sequence_length,\n",
        "                                             padding='pre'))\n",
        "    \n",
        "    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "    \n",
        "    label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
        "    \n",
        "    return predictors, label, max_sequence_length\n",
        "\n",
        "predictors, label, max_sequence_length = generate_padded_sequences(text_sequences)\n",
        "print(f\"Total number of words: {total_words} | Maximum sequence length: {max_sequence_length}\")\n",
        "print(predictors[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 3245 | Maximum sequence length: 199\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAUGAsgPMZyw",
        "colab_type": "text"
      },
      "source": [
        "We will now build the model using an Embedding layer and Stacked GRUs each with their own Dropout layer.\n",
        "\n",
        "The mask_zero in the Embedding layer tells the model to ignore the padded zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VmRF_lFFs8f",
        "colab_type": "code",
        "outputId": "8a79cf5e-04b3-4d55-87c1-ede42fb56142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "def build_model(max_sequence_length, total_words):\n",
        "    input_len = max_sequence_length - 1\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(total_words, 128, input_length=input_len, mask_zero=True))\n",
        "    model.add(SpatialDropout1D(0.5))\n",
        "    \n",
        "    model.add(GRU(512, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(GRU(512))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = build_model(max_sequence_length, total_words)\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0818 14:08:49.915250 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0818 14:08:49.920247 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0818 14:08:49.931137 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0818 14:08:49.956823 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0818 14:08:49.966783 140590955173760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0818 14:08:50.627374 140590955173760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0818 14:08:51.447117 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0818 14:08:51.476882 140590955173760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 198, 128)          415360    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 198, 128)          0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 198, 512)          984576    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 198, 512)          0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 512)               1574400   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3245)              1664685   \n",
            "=================================================================\n",
            "Total params: 4,639,021\n",
            "Trainable params: 4,639,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdS7gMHaNiDX",
        "colab_type": "text"
      },
      "source": [
        "To start training, we fit the model to the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlT4jXXVFvNT",
        "colab_type": "code",
        "outputId": "ddb14c19-ea24-4358-9b28-8f1c762c080b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training the model\n",
        "model.fit(predictors, label, batch_size=128, epochs=20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30440/30440 [==============================] - 230s 8ms/step - loss: 6.2667 - acc: 0.0847\n",
            "Epoch 2/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 5.5418 - acc: 0.1387\n",
            "Epoch 3/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 5.0598 - acc: 0.1675\n",
            "Epoch 4/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 4.6192 - acc: 0.1896\n",
            "Epoch 5/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 4.1481 - acc: 0.2128\n",
            "Epoch 6/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 3.6375 - acc: 0.2476\n",
            "Epoch 7/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 3.1532 - acc: 0.3115\n",
            "Epoch 8/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 2.7191 - acc: 0.3830\n",
            "Epoch 9/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 2.3563 - acc: 0.4511\n",
            "Epoch 10/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 2.0641 - acc: 0.5105\n",
            "Epoch 11/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 1.8159 - acc: 0.5645\n",
            "Epoch 12/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 1.5921 - acc: 0.6138\n",
            "Epoch 13/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 1.4109 - acc: 0.6556\n",
            "Epoch 14/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 1.2379 - acc: 0.7014\n",
            "Epoch 15/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 1.0927 - acc: 0.7352\n",
            "Epoch 16/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 0.9732 - acc: 0.7678\n",
            "Epoch 17/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 0.8654 - acc: 0.7947\n",
            "Epoch 18/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 0.7774 - acc: 0.8165\n",
            "Epoch 19/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 0.6986 - acc: 0.8361\n",
            "Epoch 20/20\n",
            "30440/30440 [==============================] - 224s 7ms/step - loss: 0.6418 - acc: 0.8515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd8ca68d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7raOpnVlPEt7",
        "colab_type": "text"
      },
      "source": [
        "Saving the model to use in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2A9jKL7hvSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_text_gen.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN7s5jEnPOP0",
        "colab_type": "text"
      },
      "source": [
        "The generate_text method uses a start word or phrase that is used to predict the next probable word. This word is then added to the overall output and is fed to the model again to generate the next word. This keeps on continuing till we reach the desired number of words. \n",
        "\n",
        "As our model was trained on sequences of text, we perform the same conversion before every prediction.\n",
        "\n",
        "In case we provide a word out of the model's vocabulary, we choose a random class from the top 10 classes the model predicts. We do this to avoid the same output sequence for every word not from its vocabulary list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oq8PbCSJeJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text, num_words, model, max_sequence_length):\n",
        "    for _ in range(num_words):\n",
        "        generate_random = False\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        if len(token_list) == 0:\n",
        "            generate_random = True\n",
        "        token_list = pad_sequences([token_list],\n",
        "                                   maxlen=max_sequence_length - 1,\n",
        "                                             padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        if generate_random:\n",
        "            pred_list = np.argsort(-model.predict_proba(token_list))[0]\n",
        "            predicted = pred_list[np.random.randint(low=0, high=10)]\n",
        "            print(predicted)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text.title()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig_DiqHaPJoR",
        "colab_type": "text"
      },
      "source": [
        "Let's generate some text using the function we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQSuGce4JsLa",
        "colab_type": "code",
        "outputId": "9146dfff-928e-476a-e0cc-641008b4305d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(generate_text(\"he said\", 15, model, max_sequence_length))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He Said Unto Them Go And See I Have Heard A Rumour With The Lord And Ye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfq133HQQO46",
        "colab_type": "text"
      },
      "source": [
        "Let's load our model as try predicting text using the new_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVL6lQdLMFAl",
        "colab_type": "code",
        "outputId": "42140aac-97d5-442e-815d-29ca69fa2947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "new_model = tf.keras.models.load_model('model_text_gen.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0818 15:37:35.822065 140590955173760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 198, 128)          415360    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 198, 128)          0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 198, 512)          984576    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 198, 512)          0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 512)               1574400   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3245)              1664685   \n",
            "=================================================================\n",
            "Total params: 4,639,021\n",
            "Trainable params: 4,639,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2qwCP4MTYnK",
        "colab_type": "code",
        "outputId": "2fe5ca1e-b9ad-4b9d-e204-94f7ac47ad64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(generate_text(\"he said\", 15, new_model, max_sequence_length))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He Said Unto Them Go And See I Have Heard A Rumour With The Lord And Ye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQwxj9wpXzkz",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the above output the new_model is exactly the same as the original model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu7Rfq_Ym_J",
        "colab_type": "text"
      },
      "source": [
        "A few more examples below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W13XR3KRYkax",
        "colab_type": "code",
        "outputId": "dc1fa1a1-1957-42ed-a0c9-985ae1448e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print(generate_text(\"death\", 5, model, max_sequence_length))\n",
        "print(generate_text(\"lord\", 10, model, max_sequence_length))\n",
        "print(generate_text(\"seek\", 15, model, max_sequence_length))\n",
        "print(generate_text(\"good\", 20, model, max_sequence_length))\n",
        "print(generate_text(\"silence\", 25, model, max_sequence_length))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Death Is The Lord God Of\n",
            "Lord The Eldest Day Of The Riches Both Of The Children\n",
            "Seek The Lord God Of Hosts Shall Not Be Moved And The Effect Shall Be In\n",
            "Good Thyself And The Law Is The Lord Who Is Not By The Bands Of The Lord The Rebuke Of Thy\n",
            "Silence From The People And The Lord Left And Aaron And His Sons Shall Be Like Unto His Own Neighbour As A Generations Of The Lord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apx2Rb3d-eDo",
        "colab_type": "text"
      },
      "source": [
        "Saving the tokenizer for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZrEgqByY88U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}